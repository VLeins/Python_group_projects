{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UlRbtwglU0xA",
        "outputId": "17fcf638-3b0d-486b-fa45-b40aa08b47f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'pd' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-6af75b51c3ef>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Plot terms over time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mextract_term\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mterm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m->\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \"\"\"\n\u001b[1;32m      5\u001b[0m     \u001b[0mExtracts\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mcount\u001b[0m \u001b[0mof\u001b[0m \u001b[0ma\u001b[0m \u001b[0mterm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mstring\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0madds\u001b[0m \u001b[0mit\u001b[0m \u001b[0mto\u001b[0m \u001b[0ma\u001b[0m \u001b[0mdataframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
          ]
        }
      ],
      "source": [
        "# Plot terms over time\n",
        "\n",
        "def extract_term(term:str,string:pd.DataFrame,df:pd.DataFrame,name:str)->pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Extracts the count of a term in a string and adds it to a dataframe.\n",
        "    Needed for frequency_top_terms_ts function.\n",
        "    String is column with text of pd.DataFrame\n",
        "    \"\"\"\n",
        "    string_series=pd.Series(string)\n",
        "    count=string_series.str.count(rf\"\\b{term}\\b\") # to make sure that we are only counting the word and not part of a word\n",
        "    df[f\"count_{term}_{name}\"]=count\n",
        "    return df\n",
        "\n",
        "def freq_top_terms_ts(df: pd.DataFrame, time_column: str, title_column: str, text_column: str, top_words: dict, resampling: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Plots the top terms by TF-IDF score for a subreddit.\n",
        "    If Reddit data stored as json need to convert to DataFrame first with create_posts_dataframe function.\n",
        "    Counts relative frequency of top words.\n",
        "    \"\"\"\n",
        "\n",
        "    df[time_column] = pd.to_datetime(df[time_column], unit=\"s\") # convert to datetime\n",
        "    df = df[[time_column, title_column, text_column]].copy() # keep only relevant columns\n",
        "\n",
        "    df[f\"{text_column}_processed\"] = df[text_column].apply(preprocess_text) # preprocess text\n",
        "    df[f\"{title_column}_processed\"] = df[title_column].apply(preprocess_text) # preprocess title\n",
        "\n",
        "    # Extract term counts for each word in top_words\n",
        "    for word in top_words:\n",
        "        df = extract_term(word, df[f\"{title_column}_processed\"], df, \"title\")\n",
        "        df = extract_term(word, df[f\"{text_column}_processed\"], df, \"text\")\n",
        "\n",
        "    # Calculate total word count for each post\n",
        "    df[\"total_title\"] = df[f\"{title_column}_processed\"].str.split().str.len()\n",
        "    df[\"total_text\"] = df[f\"{text_column}_processed\"].str.split().str.len()\n",
        "    df[\"total_words\"] = df[\"total_title\"] + df[\"total_text\"]\n",
        "\n",
        "    # Calculate total count for each word\n",
        "    for word in top_words:\n",
        "        df[f\"count_{word}_total\"] = df[f\"count_{word}_title\"] + df[f\"count_{word}_text\"]\n",
        "        df.drop(columns=[f\"count_{word}_title\", f\"count_{word}_text\"], inplace=True)\n",
        "\n",
        "    # Group by time and sum counts\n",
        "    df_grouped = df.resample(resampling, on=time_column).sum()\n",
        "\n",
        "    # Calculate frequency for each word in top_words\n",
        "    for word in top_words:\n",
        "        df_grouped[f\"frequency_{word}\"] = df_grouped[f\"count_{word}_total\"] / df_grouped[\"total_words\"]\n",
        "\n",
        "    return df_grouped\n",
        "\n",
        "\n",
        "def plot_freq_top_terms_ts(df_grouped:pd.DataFrame, top_words:dict, title:str) -> None:\n",
        "    \"\"\"\n",
        "    Plot the frequency of top terms by TF-IDF score in a subreddit as a time series.\n",
        "    Visualises results from freq_top_terms_ts function.\n",
        "    \"\"\"\n",
        "    # Create axis and plot time series\n",
        "    fig, ax = plt.subplots(figsize=(15, 10))\n",
        "    for word in top_words:\n",
        "        df_grouped[f\"frequency_{word}\"].plot(ax=ax, label=word)\n",
        "\n",
        "    # Format plot\n",
        "    ax.set_title(title)\n",
        "    ax.set_ylabel(\"Frequency\")\n",
        "    ax.set_xlabel(\"Date\")\n",
        "    ax.yaxis.set_major_formatter(PercentFormatter(1.0))\n",
        "    if len(df_grouped.index)<10:\n",
        "        ax.set_xticks(df_grouped.index)\n",
        "        ax.set_xticklabels(df_grouped.index.strftime('%Y-%m-%d'))\n",
        "\n",
        "    plt.legend(fontsize=14)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hypothesis 1: There will be a bigger overlap in authors that post in AskMen and TooAfraidToAsk than in AskMen and AskWomen since the majority of Reddit users are men and thus a more general subreddit like TooAfraidToAsk will have a bigger overlap with AskMen than AskWomen."
      ],
      "metadata": {
        "id": "YhEKaTjqfR-y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import packages\n",
        "from models.reddit_scraper import RedditScraper\n",
        "from config.settings import USER_AGENT\n",
        "from utils.analysis import *\n",
        "from collections import Counter\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib_venn import venn3_unweighted\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "4oyIMsF5fVKN",
        "outputId": "89af4fc8-e21b-4d1e-c676-920d994b4fce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'models'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-6d2dae4d34d5>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Import packages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreddit_scraper\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRedditScraper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettings\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mUSER_AGENT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manalysis\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'models'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get Reddit data\n",
        "scraper = RedditScraper(USER_AGENT) # create a RedditScraper object\n",
        "\n",
        "subs_of_interest = [\"AskMen\", \"AskWomen\",\"TooAfraidToAsk\"] # list of subreddits to analyze\n",
        "\n",
        "dfs = [] # list to store DataFrames\n",
        "\n",
        "for sub in subs_of_interest:\n",
        "    posts = scraper.get_subreddit_posts(sub, limit=1000,cache=True) # scrape 1000 posts#\n",
        "    dfs.append(create_posts_dataframe(posts)) # convert posts to a pandas DataFrame\n",
        "\n",
        "AskMen_df = dfs[0]\n",
        "AskWomen_df = dfs[1]\n",
        "TooAfraidToAsk_df = dfs[2]\n",
        "\n",
        "subs_of_interest_dfs = [AskMen_df, AskWomen_df, TooAfraidToAsk_df]"
      ],
      "metadata": {
        "id": "PC2Y4_HRfeU_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Filter DataFrames\n",
        "def filter_df(df:pd.DataFrame) -> pd.DataFrame:\n",
        "    df = df[(df[\"author\"] != \"[deleted]\") & (df[\"author\"] != \"AutoModerator\")] # remove deleted authors & bots\n",
        "    return df\n",
        "\n",
        "AskMen_df = filter_df(AskMen_df)\n",
        "AskWomen_df = filter_df(AskWomen_df)\n",
        "TooAfraidToAsk_df = filter_df(TooAfraidToAsk_df)\n",
        "\n",
        "\n",
        "## Get unique authors\n",
        "\n",
        "def get_unique_authors(df:pd.DataFrame) -> set:\n",
        "    return set(df[\"author\"])\n",
        "\n",
        "author_men = get_unique_authors(AskMen_df)\n",
        "author_women = get_unique_authors(AskWomen_df)\n",
        "author_tooafraidtoask = get_unique_authors(TooAfraidToAsk_df)\n",
        "\n",
        "\n",
        "print(f\"\"\"\n",
        "    Number of unique authors in AskMen: {len(author_men)}\n",
        "    Number of unique authors in AskWomen: {len(author_women)}\n",
        "    Number of unique authors in AskReddit: {len(author_tooafraidtoask)}\n",
        "    The number of unqiue authors in all three Subreddits is similiar.\n",
        "\"\"\")\n",
        "\n",
        "## Jacard Similarity\n",
        "def jaccard_similarity(set1, set2):\n",
        "    intersection = len(set1.intersection(set2))\n",
        "    union = len(set1.union(set2))\n",
        "    jaccard_similarity = intersection / union\n",
        "    return jaccard_similarity\n",
        "\n",
        "print(f\"\"\"\n",
        "    Jaccard Similarity:\n",
        "    Men & Women: {jaccard_similarity(author_men, author_women):.04f}\n",
        "    Women & TooAfraidToAsk: {jaccard_similarity(author_women, author_tooafraidtoask):.04f}\n",
        "    Men & TooAfraidToAsk: {jaccard_similarity(author_tooafraidtoask, author_men):.04f}\"\"\")\n",
        "\n",
        "v=venn3_unweighted(subsets = (author_women, author_tooafraidtoask, author_men), set_labels = (\"AskWomen\", \"TooAfraidToAsk\", \"AskMen\"))\n",
        "plt.show()\n",
        "\n",
        "print(\"\"\"\n",
        "The Jaccard Similarity shows that the distance between each Subreddit when considering the authors of each SubReddit is similiar but very small since there is barely any overlap between the authors of the Subreddits.\n",
        "However, in line with hypothesis 1, the overlap between AskMen and TooAfraidToAsk is slightly bigger than the overlap between AskWomen and TooAfraidToAsk.\n",
        "The reason why the overlap is small may be because the Reddit API only returns the last 1000 posts and hence for AskMen and TooAfraidToAsk, we only have data from the last 2 weeks (for AskWomen we can go back a bit further).\n",
        "People in these \"Question\" Subreddits may post less frequently and thus we don't observe a big overlap between the authors of the Subreddits.\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "QBY3pkryfeuZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fraction_recurring_authors(df:pd.DataFrame) -> float:\n",
        "    \"\"\"\n",
        "    Calculate the fraction of recurring authors in a Subreddit\n",
        "    \"\"\"\n",
        "    author_counts = Counter(df[\"author\"])\n",
        "    recurring_authors_count = sum(1 for count in author_counts.values() if count > 1)\n",
        "    fraction_recurring=recurring_authors_count/len(author_counts)\n",
        "    return fraction_recurring\n",
        "\n",
        "print(f\"\"\"\n",
        "    AskMen:{fraction_recurring_authors(AskMen_df):.02%}\n",
        "    AskWomen:{fraction_recurring_authors(AskWomen_df):.02%}\n",
        "    TooAfraidToAsk:{fraction_recurring_authors(TooAfraidToAsk_df):.02%}\n",
        "\n",
        "    The fraction of reoccuring authors is low in each SubReddit which supports the hypothesis that the reason we don't oberserve big overlaps may be driven by the fact that people post less frequently in \"Question\" Subreddits.\"\"\")"
      ],
      "metadata": {
        "id": "uz_2LMHyfknR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_recurring_authors(df:pd.DataFrame) -> int:\n",
        "    \"\"\"\n",
        "    Count the number of recurring authors in a Subreddit\n",
        "    \"\"\"\n",
        "    author_counts = Counter(df[\"author\"])\n",
        "    recurring_authors_count = list({author: count for author,count in author_counts.items() if count > 1}.values())\n",
        "\n",
        "    return recurring_authors_count"
      ],
      "metadata": {
        "id": "OYmeL-DbfoIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute bin edges and convert to integers\n",
        "fig, ax = plt.subplots(3, 1, figsize=(10, 15))\n",
        "\n",
        "list_counts = []\n",
        "for sub_df in [AskMen_df, AskWomen_df, TooAfraidToAsk_df]:\n",
        "    list_counts.append(count_recurring_authors(sub_df))\n",
        "\n",
        "\n",
        "for index, number in enumerate(list_counts):\n",
        "    count_bins = np.arange(2, max(number) + 2) - 0.5\n",
        "    ax[index].hist(number, bins=count_bins, edgecolor='black')\n",
        "    ax[index].set_xticks(np.arange(2, max(number) + 1))\n",
        "    ax[index].set_xlabel('Number of Posts by Author')\n",
        "    ax[index].set_ylabel('Count of Authors')\n",
        "    ax[index].set_title(f'Distribution of Recurring Authors by Number of Posts ({subs_of_interest[index]})')\n",
        "plt.subplots_adjust(hspace=0.5)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"These barplots show that even for reoccuring authors, most authors have only posted twice. This supports the hypothesis that the reason we don't observe big overlaps may be driven by the fact that people may not post frequently in \\\"Question\\\" Subreddits.\")"
      ],
      "metadata": {
        "id": "43BmSQL1fp4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hypothesis 2: There will be a bigger overlap in vocabulary that is used in AskMen and TooAfraidToAsk than in AskMen and AskWomen since the majority of Reddit users are men and thus a more general subreddit like TooAfraidToAsk will have a bigger overlap with AskMen.\n",
        "\n",
        "Hypothesis 3: In AskMen \"woman\" will be more noteworthy than \"man\" in AskWoman"
      ],
      "metadata": {
        "id": "wpYQOhTHjLg-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code for t-SNE scatterplot\n",
        "# Transform the data into headline + body text and label\n",
        "posts_df['text'] = posts_df['title'] + ' ' + posts_df['selftext']\n",
        "\n",
        "# convert text to string\n",
        "posts_df['text'] = posts_df['text'].astype(str)\n",
        "\n",
        "posts_df['tokenised_text_both'] = posts_df['text'].map(lambda x: preprocess_text_option(x, option_stopwords=\"True\", option_lemmatise=\"True\", shortword=1))\n",
        "#posts_df['tokenised_text_lemmatise'] = posts_df['text'].map(lambda x: preprocess_text_option(x, option_stopwords=\"False\", option_lemmatise=\"True\", shortword=2))\n",
        "#posts_df['tokenised_text_stopwords'] = posts_df['text'].map(lambda x: preprocess_text_option(x, option_stopwords=\"True\", option_lemmatise=\"False\", shortword=2))\n",
        "#posts_df['tokenised_text_nothing'] = posts_df['text'].map(lambda x: preprocess_text_option(x, option_stopwords=\"False\", option_lemmatise=\"False\", shortword=2))\n",
        "\n",
        "# Turn columns into a list of lists\n",
        "def column_to_list(df, column1, column2):\n",
        "    return df[[column1, column2]].values.tolist()\n",
        "\n",
        "tokenised_text_both = column_to_list(posts_df, 'tokenised_text_both', 'subreddit')\n",
        "\n",
        "# This is the tokenisation used:\n",
        "tokenised_list = tokenised_text_both\n",
        "\n",
        "corpus_text = [doc[0] for doc in tokenised_list]\n",
        "corpus_label = [doc[1] for doc in tokenised_list]\n",
        "\n",
        "vectorizer = TfidfVectorizer(min_df=2)\n",
        "tfidf_matrix = vectorizer.fit_transform(corpus_text)\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "fig_doc, ax_doc = plot_similarities(tfidf_matrix, corpus_label, \"Document Similarities (t-SNE of document vectors)\",label_color=False)\n",
        "\n",
        "# Functions from Bernie changed:\n",
        "\n",
        "def plot_similarities(tfidf_matrix, labels,\n",
        "                      title=\"term document plot\",\n",
        "                        method='tsne', is_documents=True, label_color=False,\n",
        "                      top_terms=None, figsize=(12, 8)):\n",
        "    \"\"\"\n",
        "    Create projection visualization of document or term similarities\n",
        "\n",
        "    Parameters:\n",
        "    - tfidf_matrix: scipy sparse matrix\n",
        "    - labels: list of labels (document texts or terms)\n",
        "    - title: plot title\n",
        "    - method: 'tsne' or 'mds' for dimensionality reduction\n",
        "    - top_terms: if int, only annotate top n terms\n",
        "    - is_documents: if True, plot documents, else plot terms\n",
        "    - figsize: tuple for figure size\n",
        "    \"\"\"\n",
        "\n",
        "    # Convert to dense array and transpose if visualizing terms\n",
        "    matrix = tfidf_matrix.toarray()\n",
        "    if not is_documents:\n",
        "        matrix = matrix.T\n",
        "\n",
        "    # Dimensionality reduction method\n",
        "    if method == 'tsne':\n",
        "        tsne = TSNE(n_components=2,\n",
        "                    perplexity=min(30, len(labels)-1),\n",
        "                    random_state=42,\n",
        "                    metric='cosine')\n",
        "        coords = tsne.fit_transform(matrix)\n",
        "    elif method == 'mds':\n",
        "        mds = MDS(n_components=2, dissimilarity='precomputed', random_state=42)\n",
        "        distances = 1 - cosine_similarity(matrix)\n",
        "        coords = mds.fit_transform(distances)\n",
        "    else:\n",
        "        raise ValueError(\"Method must be 'tsne' or 'mds'\")\n",
        "\n",
        "    # Create visualization\n",
        "    fig, ax = plt.subplots(figsize=figsize)\n",
        "    sns.scatterplot(x=coords[:, 0], y=coords[:, 1], alpha=0.6, hue=labels)\n",
        "\n",
        "    # Add labels\n",
        "    if top_terms and isinstance(top_terms, int):\n",
        "        mean_tfidf = tfidf_matrix.mean(axis=0).A1 if is_documents else tfidf_matrix.mean(axis=1).A1\n",
        "        top_indices = mean_tfidf.argsort()[-top_terms:][::-1]\n",
        "        labels_to_annotate = [labels[i] for i in top_indices]\n",
        "        coords_to_annotate = coords[top_indices]\n",
        "    else:\n",
        "        labels_to_annotate = labels\n",
        "        coords_to_annotate = coords\n",
        "\n",
        "    if label_color:\n",
        "        unique_labels = list(set(labels_to_annotate))\n",
        "        color_map = {label: color for label, color in zip(unique_labels, plt.cm.rainbow(np.linspace(0, 1, len(unique_labels))))}\n",
        "        colors = [color_map[label] for label in labels_to_annotate]\n",
        "    else:\n",
        "        colors = ['black'] * len(labels_to_annotate)\n",
        "\n",
        "    for i, (label, color) in enumerate(zip(labels_to_annotate, colors)):\n",
        "        # Split long labels for documents\n",
        "        if is_documents:\n",
        "            label = split_label(label, 20)\n",
        "    if  label_color:\n",
        "        ax.annotate(label, (coords_to_annotate[i, 0], coords_to_annotate[i, 1]),\n",
        "                    xytext=(5, 5), textcoords='offset points',\n",
        "                    fontsize=8 if is_documents else 12, alpha=0.7, color=color)\n",
        "\n",
        "\n",
        "    ax.set_title(title)\n",
        "    ax.grid(True, linestyle='--', alpha=0.3)\n",
        "    return fig, ax\n",
        "\n",
        "\n",
        "def preprocess_text_option(text, option_stopwords, option_lemmatise, shortword): # Different to Day 2 because also lemmatises\n",
        "    \"\"\"\n",
        "    Clean and normalize text using NLTK.\n",
        "    \"\"\"\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
        "\n",
        "    # Remove special characters and numbers\n",
        "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "\n",
        "    # Tokenize\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords\n",
        "    if option_stopwords==\"True\":\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "    # Lemmatize based on POS tag\n",
        "    if option_lemmatise==\"True\":\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "        tokens = pos_tag(tokens)\n",
        "        # Lemmatizes words either as verbs or nouns\n",
        "        tokens = [lemmatizer.lemmatize(word, 'v') if tag.startswith('V')\n",
        "                  else lemmatizer.lemmatize(word)\n",
        "                  for word, tag in tokens\n",
        "                  ]\n",
        "\n",
        "    # Remove short words\n",
        "    tokens = [token for token in tokens if len(token) > shortword]\n",
        "\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "g_Ccw6VDjj4O",
        "outputId": "d203316e-4d47-4f28-87f8-350562d0d755",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'posts_df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-84197ef0c83d>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Code for t-SNE scatterplot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Transform the data into headline + body text and label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mposts_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mposts_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'title'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mposts_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'selftext'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# convert text to string\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'posts_df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# k-means\n",
        "\n",
        "custom_cmap = ListedColormap(['#1f77b4', '#2ca02c', '#ff7f0e'])\n",
        "\n",
        "def k_means(minimum_occurrences_word: int, corpus_text: list, corpus_label: int, clusters: int, tokenpattern=r\"(?u)\\b\\w\\w+\\b\"):\n",
        "\n",
        "    \"\"\"Function to perform K-means clustering on a given dataset and plot the results\"\"\"\n",
        "\n",
        "    vectorizer = TfidfVectorizer(min_df=minimum_occurrences_word, token_pattern=tokenpattern)\n",
        "    tfidf_matrix = vectorizer.fit_transform(corpus_text)\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "    # Create and fit the k-means model\n",
        "    kmeans = KMeans(n_clusters=clusters, random_state=42)\n",
        "    cluster_labels = kmeans.fit_predict(tfidf_matrix)\n",
        "\n",
        "    # Reduce dimensionality for plotting\n",
        "    tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(feature_names)/4), metric='cosine')\n",
        "    tfidf_matrix_2d = tsne.fit_transform(tfidf_matrix.toarray())\n",
        "\n",
        "    # Shilouette score\n",
        "    silhouette_avg = silhouette_score(tfidf_matrix, cluster_labels)\n",
        "\n",
        "    # Plot the results\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    # Plot the data points, colored by their cluster assignments\n",
        "    scatter = plt.scatter(tfidf_matrix_2d[:, 0], tfidf_matrix_2d[:, 1], c=cluster_labels, cmap=custom_cmap, alpha=0.7)\n",
        "\n",
        "    # Plot the cluster centers\n",
        "    # cluster_centers_2d = tsne.fit_transform(kmeans.cluster_centers_)\n",
        "    # plt.scatter(cluster_centers_2d[:, 0],\n",
        "                # cluster_centers_2d[:, 1],\n",
        "                # c='red',\n",
        "                # marker='x',\n",
        "                # s=200,\n",
        "                # linewidth=3,\n",
        "                # label='Centroids')\n",
        "\n",
        "    plt.title(f'K-means Clustering (k={clusters})')\n",
        "\n",
        "    # Add a legend\n",
        "    legend_labels = [f'Cluster {i}' for i in range(clusters)]\n",
        "    legend_handles = scatter.legend_elements()[0]  # Get the handles for the legend\n",
        "    plt.legend(legend_handles, legend_labels, title=\"Clusters\", loc='upper right')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "     # Determine the majority label for each cluster\n",
        "    cluster_label_counts = {}\n",
        "    for cluster_label, true_label in zip(cluster_labels, corpus_label):\n",
        "        if cluster_label not in cluster_label_counts:\n",
        "            cluster_label_counts[cluster_label] = {}\n",
        "        if true_label not in cluster_label_counts[cluster_label]:\n",
        "            cluster_label_counts[cluster_label][true_label] = 0\n",
        "        cluster_label_counts[cluster_label][true_label] += 1\n",
        "\n",
        "    # Map clusters to majority labels\n",
        "    cluster_to_label = {\n",
        "      cluster: max(counts.items(), key=lambda x: x[1])[0]\n",
        "        for cluster, counts in cluster_label_counts.items()\n",
        "    }\n",
        "\n",
        "    # Convert cluster numbers to predicted labels\n",
        "    kmeans_pred = [cluster_to_label[label] for label in cluster_labels]\n",
        "\n",
        "    return kmeans_pred, cluster_label_counts, cluster_to_label, silhouette_avg\n",
        "\n",
        "kmeans_pred, cluster_label_counts,cluster_to_label, silhouette_avg = k_means(2, corpus_text, corpus_label, 3, tokenpattern=r\"(?u)\\b\\w+[-]?\\w+\\b\")\n",
        "\n",
        "print(\"\\nK-means Clustering Results:\")\n",
        "print(classification_report(corpus_label, kmeans_pred))\n",
        "print(\"\\n Cluster to Label Counts:\")\n",
        "print(cluster_label_counts)\n",
        "print(\"\\n Shilouette Score:\")\n",
        "print(silhouette_avg)"
      ],
      "metadata": {
        "id": "-KO3s67ZY27q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Zoom into woman\n",
        "\n",
        "def plot_word_associations_tsne(tfidf_matrix, feature_names, target_word='women', n_highlight=5, title=None, zoom_factor=2, jitter_strength=0):\n",
        "    \"\"\"\n",
        "    Plot word associations using t-SNE and highlight words closest to the target word after t-SNE.\n",
        "    \"\"\"\n",
        "    # Get vectors for all terms\n",
        "    term_vectors = tfidf_matrix.T.toarray()\n",
        "\n",
        "    # Calculate t-SNE for all terms\n",
        "    tsne = TSNE(n_components=2,\n",
        "                perplexity=min(30, len(feature_names) / 4),\n",
        "                random_state=42,\n",
        "                metric='cosine')\n",
        "    coords = tsne.fit_transform(term_vectors)\n",
        "\n",
        "    # Find the index of the target word\n",
        "    if target_word not in feature_names:\n",
        "        print(f\"'{target_word}' not found in feature names.\")\n",
        "        return None, None\n",
        "    target_index = np.where(feature_names == target_word)[0][0]\n",
        "\n",
        "    # Calculate distances from the target word in t-SNE space\n",
        "    distances = np.linalg.norm(coords - coords[target_index], axis=1)\n",
        "\n",
        "    # Get the indices of the closest words (excluding the target word itself)\n",
        "    closest_indices = np.argsort(distances)[1:n_highlight + 1]  # Exclude the first one, which is the word itself\n",
        "\n",
        "    # Highlight terms within the specified distance threshold\n",
        "    highlight_indices = [target_index] + closest_indices.tolist()\n",
        "\n",
        "    # Plot\n",
        "    fig, ax = plt.subplots(figsize=(10, 10))\n",
        "\n",
        "    # Plot all points in light gray\n",
        "    ax.scatter(coords[:, 0], coords[:, 1], c='lightgray', alpha=0.5, s=30)\n",
        "\n",
        "    # Highlight top terms including the target word\n",
        "    ax.scatter(coords[highlight_indices, 0], coords[highlight_indices, 1], c='#4682B4', s=100)\n",
        "\n",
        "    # Add labels for highlighted terms\n",
        "    texts = []\n",
        "    for i in highlight_indices:\n",
        "        jitter_x = np.random.uniform(-jitter_strength, jitter_strength)\n",
        "        jitter_y = np.random.uniform(-jitter_strength, jitter_strength)\n",
        "        texts.append(ax.text(coords[i, 0] + jitter_x, coords[i, 1] + jitter_y, feature_names[i], fontsize=14,\n",
        "                             bbox=dict(facecolor='white', edgecolor='gray', alpha=0.7)))\n",
        "\n",
        "    # Set limits to zoom into the area around the target word\n",
        "    x_min, x_max = coords[highlight_indices, 0].min() - zoom_factor, coords[highlight_indices, 0].max() + zoom_factor\n",
        "    y_min, y_max = coords[highlight_indices, 1].min() - zoom_factor, coords[highlight_indices, 1].max() + zoom_factor\n",
        "    ax.set_xlim(x_min, x_max)\n",
        "    ax.set_ylim(y_min, y_max)\n",
        "\n",
        "    if title:\n",
        "        ax.set_title(f'Word Associations with \"{target_word}\" in {title} (Closest {n_highlight} Words Highlighted)')\n",
        "    else:\n",
        "        ax.set_title(f'Word Associations with \"{target_word}\" (Closest {n_highlight} Words Highlighted)')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    return fig, ax\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# With what is the word \"women\" associated with in different subreddits?\n",
        "\n",
        "askmen_text= askmen_df['tokenised_text_both'].tolist()\n",
        "\n",
        "vectorizer = TfidfVectorizer(min_df=2, token_pattern=r\"(?u)\\b\\w+[-]?\\w+\\b\") # allow for hyphens in words\n",
        "tfidf_matrix = vectorizer.fit_transform(askmen_text)\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "plot_word_associations_tsne(tfidf_matrix, feature_names,target_word='woman', n_highlight=15, title=\"AskMen\", zoom_factor=3, jitter_strength=0.75)\n",
        "\n",
        "askwomen_text= askwomen_df['tokenised_text_both'].tolist()\n",
        "\n",
        "vectorizer = TfidfVectorizer(min_df=2, token_pattern=r\"(?u)\\b\\w+[-]?\\w+\\b\") # allow for hyphens in words\n",
        "tfidf_matrix = vectorizer.fit_transform(askwomen_text)\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "plot_word_associations_tsne(tfidf_matrix, feature_names,target_word='woman', n_highlight=15, title=\"AskWomen\", zoom_factor=3, jitter_strength=0.35)\n",
        "\n",
        "tooafraid_text= tooafraid_df['tokenised_text_both'].tolist()\n",
        "\n",
        "vectorizer = TfidfVectorizer(min_df=2, token_pattern=r\"(?u)\\b\\w+[-]?\\w+\\b\") # allow for hyphens in words\n",
        "tfidf_matrix = vectorizer.fit_transform(tooafraid_text)\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "plot_word_associations_tsne(tfidf_matrix, feature_names,target_word='woman', n_highlight=15, title=\"TooAfraidToAsk\", zoom_factor=3, jitter_strength=0.6)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QZf5a06zNcv3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}