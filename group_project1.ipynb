{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group Project 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define articles we want to download\n",
    "article1 = \"Liz Truss\"\n",
    "article2 = \"Liz Truss Lettuce\"\n",
    "\n",
    "# Create necessary directories if they don't exist\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "os.makedirs(\"DataFrames\", exist_ok=True)\n",
    "\n",
    "# Download revisions for both articles\n",
    "print(\"Downloading revisions for first article...\")\n",
    "os.system(f'python download_wiki_revisions.py \"{article1}\"')\n",
    "print(\"\\nDownloading revisions for second article...\")\n",
    "os.system(f'python download_wiki_revisions.py \"{article2}\"')\n",
    "\n",
    "# Convert all downloaded revisions to DataFrames\n",
    "print(\"\\nConverting revisions to DataFrames...\")\n",
    "os.system('python xml_to_dataframe.py --data-dir ./data --output-dir ./DataFrames') \n",
    "# add batch size as int --batch-size\n",
    "# include full text --include-text\n",
    "\n",
    "# Load and verify one of the DataFrames\n",
    "print(\"\\nVerifying DataFrame contents...\")\n",
    "df = pd.read_feather(f\"DataFrames/{article1}.feather\")\n",
    "\n",
    "# Display basic information about the DataFrame\n",
    "print(\"\\nDataFrame Info:\")\n",
    "print(df.info())\n",
    "\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "\n",
    "# Display some basic statistics\n",
    "print(\"\\nBasic statistics:\")\n",
    "print(f\"Total number of revisions: {len(df)}\")\n",
    "print(f\"Date range: from {df['timestamp'].min()} to {df['timestamp'].max()}\")\n",
    "print(f\"Number of unique editors: {df['username'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "import matplotlib.dates as mdates\n",
    "from scipy.stats import spearmanr\n",
    "from scipy.stats import pearsonr\n",
    "import numpy as np\n",
    "import matplotlib.colors as mcolors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Info\n",
    "# Liz Truss in office (6.9.2022-25.10.2022)\n",
    "# Wikipedia page created for meme (20.10.2022)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input Datasets\n",
    "liz_truss_df=pd.read_feather(Path().cwd()/\"DataFrames\"/\"Liz Truss.feather\")\n",
    "lettuce_df=pd.read_feather(Path().cwd()/\"DataFrames\"/\"Liz Truss lettuce.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No row is missing a timestap or revision_id so won't drop any rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by day and month\n",
    "\n",
    "lettuce_df_month=lettuce_df[[\"revision_id\",\"timestamp\"]].resample(\"M\",on=\"timestamp\").count().reset_index()\n",
    "lettuce_df_day=lettuce_df[[\"revision_id\",\"timestamp\"]].resample(\"D\",on=\"timestamp\").count().reset_index()\n",
    "liz_truss_df_month=liz_truss_df[[\"revision_id\",\"timestamp\"]].resample(\"M\",on=\"timestamp\").count().reset_index()\n",
    "liz_truss_df_day=liz_truss_df[[\"revision_id\",\"timestamp\"]].resample(\"D\",on=\"timestamp\").count().reset_index()\n",
    "lettuce_df_week=lettuce_df[[\"revision_id\",\"timestamp\"]].resample(\"W\",on=\"timestamp\").count().reset_index()\n",
    "liz_truss_df_week=liz_truss_df[[\"revision_id\",\"timestamp\"]].resample(\"W\",on=\"timestamp\").count().reset_index()\n",
    "lettuce_df_hour=lettuce_df[[\"revision_id\",\"timestamp\"]].resample(\"h\",on=\"timestamp\").count().reset_index()\n",
    "liz_truss_df_hour=liz_truss_df[[\"revision_id\",\"timestamp\"]].resample(\"h\",on=\"timestamp\").count().reset_index()\n",
    "lettuce_df_min=lettuce_df[[\"revision_id\",\"timestamp\"]].resample(\"30min\",on=\"timestamp\").count().reset_index()\n",
    "liz_truss_df_min=liz_truss_df[[\"revision_id\",\"timestamp\"]].resample(\"30min\",on=\"timestamp\").count().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format timestamp column\n",
    "\n",
    "months_list=[lettuce_df_month,liz_truss_df_month]\n",
    "days_list=[lettuce_df_day,liz_truss_df_day]\n",
    "weeks_list=[liz_truss_df_week,lettuce_df_week]\n",
    "liz_truss_list=[liz_truss_df_day,liz_truss_df_month,liz_truss_df_week]\n",
    "lettuce_list=[lettuce_df_day,lettuce_df_month,lettuce_df_week]\n",
    "\n",
    "#for df in months_list:\n",
    "#    df[\"timestamp\"]=pd.to_datetime(df[\"timestamp\"])\n",
    "#    df[\"timestamp\"] = df[\"timestamp\"].dt.strftime(\"%Y-%m\")\n",
    "\n",
    "#for df in days_list:\n",
    "#    df[\"timestamp\"]=pd.to_datetime(df[\"timestamp\"])\n",
    "#    df[\"timestamp\"] = df[\"timestamp\"].dt.strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine\n",
    "for df in liz_truss_list:\n",
    "    df[\"source\"]=\"Liz Truss\"\n",
    "\n",
    "for df in lettuce_list:\n",
    "    df[\"source\"]=\"Lettuce\"\n",
    "\n",
    "months_df=pd.concat(months_list)\n",
    "#days_df=pd.concat(days_list)\n",
    "#weeks_df=pd.concat(weeks_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge\n",
    "\n",
    "weeks_df=liz_truss_df_week.merge(lettuce_df_week,on=\"timestamp\",how=\"outer\")\n",
    "weeks_df.rename(columns={\"revision_id_x\": \"revisions_liz\", \"revision_id_y\": \"revisions_lettuce\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overview plot\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "sns.scatterplot(data=months_df,x=\"timestamp\",y=\"revision_id\",hue=\"source\")\n",
    "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter(\"%Y-%m\"))\n",
    "plt.gca().xaxis.set_major_locator(mdates.MonthLocator(interval=2))\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlim(pd.to_datetime(\"2022-01-01\"), pd.to_datetime(\"2024-12-31\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot for presentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For presentation\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "sns.scatterplot(data=weeks_df,x=\"timestamp\",y=\"revisions_liz\",alpha=0.7,color=\"#5d3eb3\",label=\"Liz Truss\")\n",
    "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter(\"%Y-%m\"))\n",
    "plt.gca().xaxis.set_major_locator(mdates.MonthLocator(interval=2))\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlim(pd.to_datetime(\"2022-01-01\"), pd.to_datetime(\"2024-12-31\"))\n",
    "ax.set_ylabel(\"Count of Revisions to Liz Trust page (week)\")\n",
    "ax.set_xlabel(None)\n",
    "\n",
    "# Secound y-axis\n",
    "ax2 = plt.gca().twinx()\n",
    "sns.scatterplot(data=weeks_df,x=\"timestamp\",y=\"revisions_lettuce\",ax=ax2,color=\"#32a852\", alpha=0.7,label=\"Lettuce Meme\")\n",
    "ax2.set_ylabel(\"Count of Revisions to Lettuce Meme page (week)\")\n",
    "\n",
    "# Vertical lines\n",
    "resignation = pd.to_datetime(\"2022-10-20\")\n",
    "office= pd.to_datetime(\"2022-09-05\")\n",
    "ax.axvline(resignation, color=\"#F08080\", linestyle='--', linewidth=1.5, label=\"Resignation\")\n",
    "ax.axvline(office, color=\"lightblue\", linestyle='--', linewidth=1.5, label=\"Appointment\")\n",
    "\n",
    "# Combine legends from both axes\n",
    "handles, labels = ax.get_legend_handles_labels()  # Get handles & labels from the first axis\n",
    "handles2, labels2 = ax2.get_legend_handles_labels()  # Get handles & labels from the second axis\n",
    "handles.extend(handles2)  # Combine handles\n",
    "labels.extend(labels2)  # Combine labels\n",
    "ax2.get_legend().remove() # Remove ax2 legend\n",
    "\n",
    "# Create combined legend\n",
    "ax.legend(handles, labels, title=\"Legend\", loc=\"upper right\")\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lagged correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_with_lagged_x(y:pd.Series, x:pd.Series, max_lag:int) -> list:\n",
    "    '''Calculate Spearman correlation between y and x where x is lagged by 0 until mag_lag periods''' \n",
    "    statistic=[]\n",
    "    pvalue=[]\n",
    "    n = len(y)\n",
    "    for lag in range(max_lag + 1):\n",
    "        y_truncated = y[:n - lag]  # Truncate y\n",
    "        x_shifted = x[lag:]         # Shift x by lag\n",
    "        corr = spearmanr(y_truncated, x_shifted)\n",
    "        statistic.append(corr.statistic)\n",
    "        pvalue.append(corr.pvalue)\n",
    "    return statistic,pvalue\n",
    "\n",
    "def heatmap_for_correlation(correlation:list,pvalue:list,column_name:str,max_lag:int) -> None:\n",
    "    '''Creates a heatmap to visualise the correlations between y and lagged x'''\n",
    "    correlation = np.array(correlation).reshape(1, -1)  # Reshape for heatmap\n",
    "    pvalue = np.array(pvalue).reshape(1, -1)  # Reshape for heatmap\n",
    "    heatmap_df = pd.DataFrame(correlation,  # Stack p-values and statistics vertically\n",
    "        index=[\"Spearman Correlation\"],  # Labels for the two rows\n",
    "        columns=[f'{column_name} {lag}' for lag in range(max_lag + 1)]\n",
    "        )\n",
    "    \n",
    "    return heatmap_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_df=liz_truss_df_day.merge(lettuce_df_day,on=\"timestamp\",how=\"outer\") \\\n",
    "                        .rename(columns={\"revision_id_x\": \"revisions_liz\", \"revision_id_y\": \"revisions_lettuce\"}) \\\n",
    "                        .dropna() \\\n",
    "                        .sort_values(by=\"timestamp\", ascending=True)\n",
    "# merge dataframes, rename columns, drop all days before lettuce meme was created & sort dataframe based on dates\n",
    "\n",
    "max_lag=10\n",
    "\n",
    "statistic,pvalue=correlation_with_lagged_x(day_df[\"revisions_lettuce\"],day_df[\"revisions_liz\"],max_lag)\n",
    "\n",
    "heatmap_df=heatmap_for_correlation(statistic,pvalue,\"Day\",max_lag)\n",
    "\n",
    "\n",
    "# Plotting Heatmap\n",
    "plt.figure(figsize=(15, 5))\n",
    "sns.heatmap(heatmap_df, annot=True, cmap='coolwarm',linewidths=0.5, linecolor='gray',fmt=\".2f\")\n",
    "plt.title('Pearson Correlation between Counts of Meme Revisions and Lagged Counts of Liz Truss Revisions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for correlations\n",
    "hour_df=liz_truss_df_hour.merge(lettuce_df_hour,on=\"timestamp\",how=\"outer\") \\\n",
    "                        .rename(columns={\"revision_id_x\": \"revisions_liz\", \"revision_id_y\": \"revisions_lettuce\"}) \\\n",
    "                        .dropna() \\\n",
    "                        .sort_values(by=\"timestamp\", ascending=True)\n",
    "# merge dataframes, rename columns, drop all days before lettuce meme was created & sort dataframe based on dates\n",
    "\n",
    "max_lag=12\n",
    "\n",
    "statistic,pvalue=correlation_with_lagged_x(hour_df[\"revisions_lettuce\"],hour_df[\"revisions_liz\"],max_lag)\n",
    "\n",
    "heatmap_df=heatmap_for_correlation(statistic,pvalue,\"Hour\",max_lag)\n",
    "\n",
    "\n",
    "# Plotting Heatmap\n",
    "plt.figure(figsize=(10, 4))\n",
    "sns.heatmap(heatmap_df, annot=True, cmap='coolwarm',linewidths=0.5, linecolor='gray',fmt=\".2f\",cbar=False)\n",
    "plt.title('Spearman Correlation between Counts of Meme Revisions and Lagged Counts of Liz Truss Revisions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for correlations\n",
    "hour_df=liz_truss_df_hour.merge(lettuce_df_hour,on=\"timestamp\",how=\"outer\") \\\n",
    "                        .rename(columns={\"revision_id_x\": \"revisions_liz\", \"revision_id_y\": \"revisions_lettuce\"}) \\\n",
    "                        .dropna() \\\n",
    "                        .sort_values(by=\"timestamp\", ascending=True)\n",
    "# merge dataframes, rename columns, drop all days before lettuce meme was created & sort dataframe based on dates\n",
    "\n",
    "max_lag=12\n",
    "\n",
    "statistic,pvalue=correlation_with_lagged_x(hour_df[\"revisions_liz\"],hour_df[\"revisions_lettuce\"],max_lag)\n",
    "\n",
    "heatmap_df=heatmap_for_correlation(statistic,pvalue,\"Hour\",max_lag)\n",
    "\n",
    "\n",
    "# Plotting Heatmap\n",
    "plt.figure(figsize=(10, 4))\n",
    "sns.heatmap(heatmap_df, annot=True, cmap='coolwarm',linewidths=0.5, linecolor='gray',fmt=\".2f\",cbar=False)\n",
    "plt.title('Spearman Correlation between Counts of Liz Truss Revisions and Lagged Counts of Meme Revisions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Author Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Who edits which page?\n",
    "# Authors of the lettuce meme Wikipedia page exhibit a higher fraction of \"recurring\" authors compared to the Liz Truss Wikipedia page, while the Liz Truss page has a higher fraction of \"once\" authors compared to the meme page\n",
    "# A small community of the same people will edit the meme while there are only a few \"once-a-time\" authors since the meme is a niche page on Wikipedia not many people may know about\n",
    "# A big community of people will edit the Liz Truss page with a small amount of people that do it recurringly since Liz Truss is a well-known politician\n",
    "\n",
    "\n",
    "# Restrict liz_truss_df to time where meme page existed\n",
    "liz_truss_df_recent=liz_truss_df[liz_truss_df[\"timestamp\"] >= lettuce_df.sort_values(by=\"timestamp\", ascending=True)[\"timestamp\"][0]]\n",
    "\n",
    "# Drop rows with missing usernames\n",
    "liz_truss_df_recent.dropna(subset=\"username\",inplace=True)\n",
    "lettuce_df.dropna(subset=\"username\",inplace=True)\n",
    "\n",
    "# Determine author groups\n",
    "liz_truss_editor=set(liz_truss_df_recent[\"username\"])\n",
    "lettuce_editor=set(lettuce_df[\"username\"])\n",
    "common_editor=liz_truss_editor & lettuce_editor\n",
    "only_liz_truss_editor=liz_truss_editor-lettuce_editor\n",
    "only_lettuce_editor=lettuce_editor-liz_truss_editor\n",
    "\n",
    "# Count how many revision per author\n",
    "liz_truss_counts=liz_truss_df_recent[[\"username\",\"revision_id\"]].groupby(\"username\").count().rename(columns={\"revision_id\":\"counts\"})\n",
    "lettuce_counts=lettuce_df[[\"username\",\"revision_id\"]].groupby(\"username\").count().rename(columns={\"revision_id\":\"counts\"})\n",
    "\n",
    "# Create user category\n",
    "dfs=[liz_truss_counts,lettuce_counts]\n",
    "\n",
    "for df in dfs:\n",
    "    df.loc[df.index.isin(only_liz_truss_editor), \"user_category\"] = \"Liz Truss\"\n",
    "    df.loc[df.index.isin(only_lettuce_editor), \"user_category\"] = \"Lettuce\"\n",
    "    df.loc[df.index.isin(common_editor), \"user_category\"] = \"Both\"\n",
    "\n",
    "# Define if \"one-time\" or \"recurring\" author\n",
    "liz_truss_counts[\"user_type\"] = liz_truss_counts[\"counts\"].apply(lambda x: \"Recurring\" if x > 1 else \"Once\")\n",
    "lettuce_counts[\"user_type\"] = lettuce_counts[\"counts\"].apply(lambda x: \"Recurring\" if x > 1 else \"Once\")\n",
    "lettuce_counts[\"page\"]=\"Lettuce\"\n",
    "liz_truss_counts[\"page\"] =\"Liz Truss\"\n",
    "\n",
    "# merge lettuce and Liz Truss\n",
    "df_merged=pd.concat([liz_truss_counts,lettuce_counts])\n",
    "\n",
    "# Carry out chi2\n",
    "chi2, p, dof, expected=chi2_contingency(pd.crosstab(df_merged[\"page\"],df_merged[\"user_type\"]))\n",
    "expected_df = pd.DataFrame(expected, columns=[\"Once\", \"Recurring\"], index=[\"Lettuce\", \"Liz Truss\"]).astype(int)\n",
    "\n",
    "print(f\"We reject the H0 (distribution of user types is the same across the meme and Liz Truss page) since the p-value is {p}.\")\n",
    "\n",
    "print(f\"\\nActual distribution: \\n{pd.crosstab(df_merged['page'],df_merged['user_type'])}\")\n",
    "\n",
    "print(f\"\\nExpected distribution: \\n{expected_df}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
