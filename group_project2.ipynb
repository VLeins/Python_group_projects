{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload \n",
    "%autoreload 2 \n",
    "# all imported modules will be automatically reloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "from models.reddit_scraper import RedditScraper\n",
    "from config.settings import USER_AGENT\n",
    "from utils.analysis import *\n",
    "from utils.network_builder import *\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib_venn import venn3_unweighted, venn3\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get post data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Reddit data\n",
    "scraper = RedditScraper(USER_AGENT) # create a RedditScraper object\n",
    "\n",
    "subs_of_interest = [\"AskMen\", \"AskWomen\",\"TooAfraidToAsk\"] # list of subreddits to analyze\n",
    "\n",
    "dfs = [] # list to store DataFrames\n",
    "\n",
    "for sub in subs_of_interest:\n",
    "    posts = scraper.get_subreddit_posts(sub, limit=100,cache=True) # scrape 1000 posts#\n",
    "    dfs.append(create_posts_dataframe(posts)) # convert posts to a pandas DataFrame\n",
    "\n",
    "AskMen_df = dfs[0]\n",
    "AskWomen_df = dfs[1]\n",
    "TooAfraidToAsk_df = dfs[2]\n",
    "\n",
    "subs_of_interest_dfs = [AskMen_df, AskWomen_df, TooAfraidToAsk_df]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Author Analysis\n",
    "\n",
    "There will be a bigger overlap in authors that post in AskMen and TooAfraidToAsk than in AskMen and AskWomen since the majority of Reddit users are men and thus a more general subreddit like TooAfraidToAsk will have a bigger overlap with AskMen than AskWomen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Filter DataFrames\n",
    "def filter_df(df:pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df[(df[\"author\"] != \"[deleted]\") & (df[\"author\"] != \"AutoModerator\")] # remove deleted authors & bots\n",
    "    return df\n",
    "\n",
    "AskMen_df = filter_df(AskMen_df)\n",
    "AskWomen_df = filter_df(AskWomen_df)\n",
    "TooAfraidToAsk_df = filter_df(TooAfraidToAsk_df)\n",
    "\n",
    "\n",
    "## Get unique authors\n",
    "\n",
    "def get_unique_authors(df:pd.DataFrame) -> set:\n",
    "    return set(df[\"author\"])\n",
    "\n",
    "author_men = get_unique_authors(AskMen_df)\n",
    "author_women = get_unique_authors(AskWomen_df)\n",
    "author_tooafraidtoask = get_unique_authors(TooAfraidToAsk_df)\n",
    "\n",
    "\n",
    "print(f\"\"\"\n",
    "    Number of unique authors in AskMen: {len(author_men)}\n",
    "    Number of unique authors in AskWomen: {len(author_women)}\n",
    "    Number of unique authors in TooAfraidToAsk: {len(author_tooafraidtoask)}\n",
    "    The number of unqiue authors in all three Subreddits is similiar.\n",
    "\"\"\")\n",
    "\n",
    "## Jacard Similarity\n",
    "def jaccard_similarity(set1, set2):\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    union = len(set1.union(set2))\n",
    "    jaccard_similarity = intersection / union\n",
    "    return jaccard_similarity\n",
    "\n",
    "print(f\"\"\" \n",
    "    Jaccard Similarity:\n",
    "    Men & Women: {jaccard_similarity(author_men, author_women):.04f}\n",
    "    Women & TooAfraidToAsk: {jaccard_similarity(author_women, author_tooafraidtoask):.04f}\n",
    "    Men & TooAfraidToAsk: {jaccard_similarity(author_tooafraidtoask, author_men):.04f}\"\"\")\n",
    "\n",
    "v=venn3(subsets = (author_women, author_tooafraidtoask, author_men), set_labels = (\"AskWomen\", \"TooAfraidToAsk\", \"AskMen\"))\n",
    "plt.show()\n",
    "\n",
    "print(\"\"\"\n",
    "The Jaccard Similarity shows that the distance between each Subreddit when considering the authors of each SubReddit is similiar but very small since there is barely any overlap between the authors of the Subreddits.\n",
    "The reason why the overlap is small may be because the Reddit API only returns the last 1000 posts and hence for AskMen and TooAfraidToAsk, we only have data from the last 2 weeks (for AskWomen we can go back a bit further).\n",
    "People in these \"Question\" Subreddits may post less frequently and thus we don't observe a big overlap between the authors of the Subreddits.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recurring_authors(df:pd.DataFrame, sort:str) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the fraction or sum of recurring authors in a dataframe\n",
    "    \"\"\"\n",
    "    author_counts = Counter(df[\"author\"])\n",
    "    recurring_authors_count = sum(1 for count in author_counts.values() if count > 1)\n",
    "    if sort == \"fraction\":\n",
    "        fraction_recurring=recurring_authors_count/len(author_counts)\n",
    "        return fraction_recurring\n",
    "    elif sort == \"count\":\n",
    "        return recurring_authors_count\n",
    "    else:\n",
    "        return \"Invalid sort parameter. Choose 'fraction' or 'count'\"\n",
    "\n",
    "print(f\"\"\"\n",
    "    Fraction of recurring authors:\n",
    "    AskMen:{recurring_authors(AskMen_df, sort=\"fraction\"):.02%}\n",
    "    AskWomen:{recurring_authors(AskWomen_df, sort=\"fraction\"):.02%}\n",
    "    TooAfraidToAsk:{recurring_authors(TooAfraidToAsk_df, sort=\"fraction\"):.02%}\n",
    "\n",
    "    The fraction of reoccuring authors is low in each SubReddit which supports the hypothesis that the reason we don't oberserve big overlaps may be driven by the fact that people post less frequently in \"Question\" Subreddits.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute bin edges and convert to integers\n",
    "fig, ax = plt.subplots(3, 1, figsize=(10, 15))\n",
    "\n",
    "list_counts = []\n",
    "for sub_df in [AskMen_df, AskWomen_df, TooAfraidToAsk_df]:\n",
    "    author_counts = Counter(sub_df[\"author\"])\n",
    "    recurring_counts = [count for count in author_counts.values() if count > 1]\n",
    "    list_counts.append(recurring_counts)\n",
    "\n",
    "for index, counts in enumerate(list_counts):\n",
    "    count_bins = np.arange(2, max(counts) + 2) - 0.5\n",
    "    ax[index].hist(counts, bins=count_bins, edgecolor='black')\n",
    "    ax[index].set_xticks(np.arange(2, max(counts) + 1))\n",
    "    ax[index].set_xlabel('Number of Posts by Author')\n",
    "    ax[index].set_ylabel('Count of Authors')\n",
    "    ax[index].set_title(f'Distribution of Recurring Authors by Number of Posts ({subs_of_interest[index]})')\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"These barplots show that even for reoccuring authors, most authors have only posted twice. This supports the hypothesis that the reason we don't observe big overlaps may be driven by the fact that people may not post frequently in \\\"Question\\\" Subreddits.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comment Depth Analysis\n",
    "AskMen and TooAfraidToAsk may, on average, have shorter comment trees due to a focus on direct advice and minimal follow-up engagement, whereas AskWomen encourage more elaborate discussions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#askmen_comments_posts, askmen_comments_df = get_comments_df(\"AskMen\", sort=\"hot\",user_agent=USER_AGENT, limit=100)\n",
    "tata_comments_posts, tata_comments_df = get_comments_df(\"TooAfraidToAsk\", sort=\"hot\",user_agent=USER_AGENT, limit=5)\n",
    "\n",
    "#askwomen_comments_posts, askwomen_comments_df = get_comments_df(\"AskWomen\", sort=\"hot\",user_agent=USER_AGENT, limit=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comment_depth(df,posts,limit=100):\n",
    "    \"\"\"\n",
    "    Calculate the depth of the comment tree for each post in the dataframe\n",
    "\n",
    "    Inputs:\n",
    "    - df: DataFrame containing comments\n",
    "    - posts: list of dictionaries containing post information\n",
    "\n",
    "    \"\"\"\n",
    "    depth_list = []\n",
    "    for i in range(0,limit-1):\n",
    "        post_comments = df[df['post_id'] == posts[i]['id']]\n",
    "        comment_tree = usercomment_tree(post_comments, include_root=True)\n",
    "        depth=depth = nx.dag_longest_path_length(comment_tree)\n",
    "        depth_list.append(depth)\n",
    "    return depth_list\n",
    "\n",
    "depth_askmen = comment_depth(askmen_comments_df,askmen_comments_posts,limit=100)\n",
    "depth_askwomen = comment_depth(askwomen_comments_df,askwomen_comments_posts,limit=100)\n",
    "depth_tata= comment_depth(tata_comments_df,tata_comments_posts,limit=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_list(list):\n",
    "    return sum(list)/len(list)\n",
    "\n",
    "print(f\"Mean comment depth for AskMen: {mean_list(depth_askmen):.02f}\")\n",
    "print(f\"Mean comment depth for AskWomen: {mean_list(depth_askwomen):.02f}\")\n",
    "print(f\"Mean comment depth for TooAfraidToAsk: {mean_list(depth_tata):.02f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anova test\n",
    "from scipy.stats import f_oneway\n",
    "f_stat, p_val = f_oneway(depth_askmen,depth_askwomen,depth_tata)\n",
    "\n",
    "if p_val < 0.05:\n",
    "    print(f\"Reject the null hypothesis that the mean comment depth is the same for all Subreddits (p-value: {p_val:.04f})\")\n",
    "else:\n",
    "    print(f\"Fail to reject the null hypothesis that the mean comment depth is the same for all Subreddits (p-value: {p_val:.04f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post hoc test (if reject H0)\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "\n",
    "# Combine all depths into a single list and create corresponding labels\n",
    "all_depths = depth_askmen + depth_askwomen + depth_tata\n",
    "labels = ['AskMen'] * len(depth_askmen) + ['AskWomen'] * len(depth_askwomen) + ['TooAfraidToAsk'] * len(depth_tata)\n",
    "\n",
    "# Perform Tukey's HSD test\n",
    "tukey = pairwise_tukeyhsd(endog=all_depths, groups=labels, alpha=0.05)\n",
    "print(tukey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualse mean depth of comment tree\n",
    "fig, ax = plt.subplots(1, 3, figsize=(30, 10))\n",
    "sns.histplot(depth_askmen, bins=max(depth_askmen), color='blue', alpha=0.5, label='AskMen', ax=ax[0])\n",
    "sns.histplot(depth_askwomen, bins=max(depth_askwomen), color='red', alpha=0.5, label='AskWomen', ax=ax[1])\n",
    "sns.histplot(depth_tata, bins=max(depth_tata), color='green', alpha=0.5, label='TooAfraidToAsk', ax=ax[2])\n",
    "\n",
    "for a in ax:\n",
    "\ta.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the data into headline + body text and label\n",
    "posts_df['text'] = posts_df['title'] + ' ' + posts_df['selftext']\n",
    "\n",
    "# convert text to string\n",
    "posts_df['text'] = posts_df['text'].astype(str)\n",
    "\n",
    "# Tokenize the data\n",
    "\n",
    "posts_df['tokenised_text_both'] = posts_df['text'].map(lambda x: preprocess_text_hyphen(x, option_stopwords=\"True\", option_lemmatise=\"True\", shortword=2))\n",
    "posts_df['tokenised_text_lemmatise'] = posts_df['text'].map(lambda x: preprocess_text_hyphen(x, option_stopwords=\"False\", option_lemmatise=\"True\", shortword=2))\n",
    "\n",
    "# Keeping hyphens so words such as ex-boyfriend or make-up are kept together. Also keeps words such as full-time or self-esteem together\n",
    "\n",
    "# Turn columns into a list of lists\n",
    "def column_to_list(df, column1, column2):\n",
    "    \"\"\" \n",
    "    Convert two columns of a dataframe into a list of lists in order to use them in a text processing exercise where need label and text.\n",
    "    \"\"\"\n",
    "    return df[[column1, column2]].values.tolist()\n",
    "\n",
    "tokenised_text_both = column_to_list(posts_df, 'tokenised_text_both', 'subreddit')\n",
    "tokenised_text_lemmatise = column_to_list(posts_df, 'tokenised_text_lemmatise', 'subreddit')\n",
    "\n",
    "# Save each subreddit in df \n",
    "askwomen_df= posts_df[posts_df['subreddit'] == 'AskWomen']\n",
    "askmen_df= posts_df[posts_df['subreddit'] == 'AskMen']\n",
    "tooafraid_df= posts_df[posts_df['subreddit'] == 'TooAfraidtoask']\n",
    "\n",
    "# This is the tokenisation used:\n",
    "tokenised_list = tokenised_text_both\n",
    "\n",
    "# Split the list into text and label\n",
    "corpus_text = [doc[0] for doc in tokenised_list]\n",
    "corpus_label = [doc[1] for doc in tokenised_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot post similarities (t-SNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot t-SNE similiarity of posts\n",
    "vectorizer = TfidfVectorizer(min_df=2, token_pattern=r\"(?u)\\b\\w+[-]?\\w+\\b\") # allow for hyphens in words\n",
    "tfidf_matrix = vectorizer.fit_transform(corpus_text)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "fig_doc, ax_doc = plot_similarities(tfidf_matrix, corpus_label, \"Post Similarities without stop words (t-SNE of post vectors)\",label_color=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The t-SNE plot shows a lot of overlap between all three Subreddits, indicating that similar topics may be discussed across Subreddits.\n",
    "However, it seems like that there may be a topic or group of topics that are only discussed in the AskMen and TooAfraidToAsk since there is one cluster on the left side containing AskMen and TooAfraidToAsk posts but not contain many AskWomen posts, which is in line with our hypothesis that AskMen and TooAfraidToAsk are less distinct than AskWomen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the tokenisation used:\n",
    "tokenised_list = tokenised_text_lemmatise\n",
    "\n",
    "# Split the list into text and label\n",
    "corpus_text = [doc[0] for doc in tokenised_list]\n",
    "corpus_label = [doc[1] for doc in tokenised_list]\n",
    "\n",
    "# Plot t-SNE similiarity of posts\n",
    "vectorizer = TfidfVectorizer(min_df=2, token_pattern=r\"(?u)\\b\\w+[-]?\\w+\\b\") # allow for hyphens in words\n",
    "tfidf_matrix = vectorizer.fit_transform(corpus_text)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "fig_doc, ax_doc = plot_similarities(tfidf_matrix, corpus_label, \"Document Similarities with stop words (t-SNE of document vectors)\",label_color=False)\n",
    "\n",
    "# Better clustering when not removing stop words -> subreddits use different stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When not removing stop words the clustering seems to be more distinct and in line with our intial hypothesis since we can now differentiate between a AskWomen cluster and a mixed AskMen and TooAfraidToAsk cluster. This supports our hypothesis that stopwords may help to differentiate between subreddits due to using them in different ways. \n",
    "\n",
    " We then carry out a Chi2 to examine whether the relative frequency of \"and\" is statistically different across the 3 subreddits and find that AskWomen uses \"and\" relatively less than AskMen and TooAfraidToAsk. Hence, this is against our hypothesis and may be driven by the fact that men maybe write more informally than women and thus use more \"and\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chi2 and post-hoc chi2 for word frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Quantify difference in the use of \"and\"\n",
    "\n",
    "# Count the number of times \"and\" appears in the text and how many words there are\n",
    "posts_df = extract_term(\"and\", posts_df[\"tokenised_text_lemmatise\"], posts_df, \"\")\n",
    "posts_df[\"count_text\"] = posts_df[\"tokenised_text_lemmatise\"].str.split().str.len()\n",
    "posts_df[\"fraction_and_post\"] = (posts_df['count_and_'] / posts_df['count_text']) * 100\n",
    "\n",
    "# Sum counts based on subreddit\n",
    "subreddit_counts = posts_df.groupby('subreddit')[['count_and_', 'count_text']].sum()\n",
    "\n",
    "# Calculate fraction of counts in each subreddit as percentage of overall word count\n",
    "subreddit_counts['fraction_and'] = (subreddit_counts['count_and_'] / subreddit_counts['count_text']) * 100\n",
    "subreddit_counts['fraction_and'] = subreddit_counts['fraction_and'].round(2)\n",
    "\n",
    "subreddit_counts.rename(columns={'count_and_': 'Count of \"and\"', 'count_text': 'Word count', 'fraction_and': 'Fraction of \"and\" (%)'}, inplace=True)\n",
    "\n",
    "print(subreddit_counts[['Count of \"and\"', 'Word count', 'Fraction of \"and\" (%)']])\n",
    "\n",
    "# Create a contingency table\n",
    "contingency_table = subreddit_counts[['Count of \"and\"', 'Word count']].values\n",
    "\n",
    "# Perform the chi-square test\n",
    "chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
    "\n",
    "print(f\"\\nChi-square statistic: {chi2:.2f}\")\n",
    "print(f\"P-value: {p:.2f}\")\n",
    "\n",
    "# Plot count of words and counts of why on bar plot\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(12, 10))\n",
    "\n",
    "# Plot count of \"and\" on primary y-axis\n",
    "sns.boxplot(x='subreddit', y='count_and_', data=posts_df, ax=ax1)\n",
    "ax1.set_ylabel('Count of \"and\"')\n",
    "plt.show()\n",
    "\n",
    "# Post-hoc pairwise comparisons chi2 test\n",
    "\n",
    "# Subset the data for each pair of subreddits\n",
    "askmen_vs_askwomen = subreddit_counts.loc[['AskMen', 'AskWomen']]\n",
    "askmen_vs_tooafraid = subreddit_counts.loc[['AskMen', 'TooAfraidtoask']]\n",
    "askwomen_vs_tooafraid = subreddit_counts.loc[['TooAfraidtoask', 'AskWomen']]\n",
    "\n",
    "# Function to create a contingency table and perform chi2 test\n",
    "def perform_chi_square(df):\n",
    "    df=df.drop(columns=['fraction_and'])\n",
    "    contingency_table = df.values\n",
    "    chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
    "    return chi2, p, dof, expected\n",
    "\n",
    "# Perform chi-square tests for each pair\n",
    "chi2_am_aw, p_am_aw, _, _ = perform_chi_square(askmen_vs_askwomen)\n",
    "chi2_am_ta, p_am_ta, _, _ = perform_chi_square(askmen_vs_tooafraid)\n",
    "chi2_aw_ta, p_aw_ta, _, _ = perform_chi_square(askwomen_vs_tooafraid)\n",
    "\n",
    "# Set the Bonferroni-corrected significance level because we are doing 3 post-hoc tests\n",
    "alpha = 0.05\n",
    "corrected_alpha = alpha / 3 \n",
    "# Print the results\n",
    "print(f\"AskMen vs AskWomen: Chi-square = {chi2_am_aw:.2f}, p-value = {p_am_aw:.2f}\")\n",
    "print(f\"AskMen vs TooAfraidToAsk: Chi-square = {chi2_am_ta:.2f}, p-value = {p_am_ta:.2f}\")\n",
    "print(f\"AskWomen vs TooAfraidToAsk: Chi-square = {chi2_aw_ta:.2f}, p-value = {p_aw_ta:.2f}\")\n",
    "\n",
    "# Check if p-values are below the corrected alpha threshold\n",
    "print(f\"\\nCorrected alpha threshold: {corrected_alpha:.2f}\")\n",
    "print(f\"AskMen vs AskWomen statistically different? {p_am_aw < corrected_alpha}\")\n",
    "print(f\"AskMen vs TooAfraidToAsk statistically different? {p_am_ta < corrected_alpha}\")\n",
    "print(f\"AskWomen vs TooAfraidToAsk  statistically different? {p_aw_ta < corrected_alpha}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
